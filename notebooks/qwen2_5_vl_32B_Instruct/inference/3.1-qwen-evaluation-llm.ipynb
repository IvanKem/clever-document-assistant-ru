{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:01:26.086083Z",
     "iopub.status.busy": "2025-10-15T23:01:26.085085Z",
     "iopub.status.idle": "2025-10-15T23:03:01.350436Z",
     "shell.execute_reply": "2025-10-15T23:03:01.349145Z",
     "shell.execute_reply.started": "2025-10-15T23:01:26.086038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-10-15 23:02:14.590797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-15 23:02:18.718736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 23:02:46 [__init__.py:216] Automatically detected platform cuda.\n",
      "ERROR 10-15 23:02:49 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Requires Flash-Attention version >=2.7.1,<=2.8.2 but got 2.8.3.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from unsloth import FastLanguageModel\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.353464Z",
     "iopub.status.busy": "2025-10-15T23:03:01.352121Z",
     "iopub.status.idle": "2025-10-15T23:03:01.375577Z",
     "shell.execute_reply": "2025-10-15T23:03:01.374306Z",
     "shell.execute_reply.started": "2025-10-15T23:03:01.353417Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_filename = '/home/jupyter/datasphere/project/evaluation_results_fair.json'\n",
    "with open(input_filename, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "sentence_pairs = []\n",
    "for result in json_data[\"results\"]:\n",
    "  sentence_pairs.append([result[\"ground_truth\"] if type(result[\"ground_truth\"]).__name__ == \"str\" else result[\"ground_truth\"][0], result[\"prediction\"] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.377866Z",
     "iopub.status.busy": "2025-10-15T23:03:01.377196Z",
     "iopub.status.idle": "2025-10-15T23:03:01.402402Z",
     "shell.execute_reply": "2025-10-15T23:03:01.401101Z",
     "shell.execute_reply.started": "2025-10-15T23:03:01.377797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt_en(gt, pred):\n",
    "    return f\"\"\"\n",
    "Reference Answer: \"{gt}\"\n",
    "Generated Answer: \"{pred}\"\n",
    "\n",
    "Analyze these two texts and provide your rating with a justification.\n",
    "Your justification:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.406789Z",
     "iopub.status.busy": "2025-10-15T23:03:01.405299Z",
     "iopub.status.idle": "2025-10-15T23:03:01.425151Z",
     "shell.execute_reply": "2025-10-15T23:03:01.423785Z",
     "shell.execute_reply.started": "2025-10-15T23:03:01.406734Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt_en = \"\"\"You are a meticulous and strict AI evaluator. Your goal is to score the \"Generated Answer\" based on its factual accuracy and completeness when compared to the \"Reference Answer\".\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  **NON-NEGOTIABLE RULE ‚Äî Factual Accuracy is Paramount:** This rule OVERRIDES all others. Pay extremely close attention to numbers, dates, names, statistics, and other specific facts. A single factual error that contradicts the Reference Answer makes the entire answer wrong and **must result in a score of 0.0**, regardless of how well-structured or detailed the answer is.\n",
    "2.  **Completeness:** Check if the Generated Answer includes all key information present in the Reference Answer. Missing a crucial detail should lower the score.\n",
    "3.  **Correct Refusal:** If the Generated Answer correctly states that information is not available in the source document, this is a GOOD answer. It shows honesty and avoids hallucination.\n",
    "4.  **Language Invariance:** Evaluate based on meaning, regardless of the language (English, Russian, etc.). The rules are the same for all languages.\n",
    "5.  **Handling Minimalist Reference Answers:** If the Reference Answer is very short (e.g., \"Yes\", \"No\", \"–î–∞\", \"–ù–µ—Ç\", or a single number), it represents the core truth, not the full context. A Generated Answer that correctly states this core truth and adds relevant, descriptive details should be considered high-quality. Do not penalize it for providing extra information unless that information contradicts the core truth.\n",
    "\n",
    "First, write a brief justification for your assessment in 1-2 sentences. After the justification, on a new line, write \"Rating:\" followed by your score on a scale from 0 to 5, with a step of 0.25.\n",
    "\n",
    "**CRITICAL OUTPUT FORMATTING RULE:** Regardless of the language of the texts being evaluated (English, Russian, etc.), your justification must be in English, and the final line with the score **MUST ALWAYS** use the exact format `Rating: [score]`.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5.0:** Perfect match. All facts are correct and all key information from the reference is included.\n",
    "- **4.0 - 4.75:** Factually correct, but might miss a very minor detail or include extra, harmless information.\n",
    "- **2.0 - 3.75:** Partially correct. Captures the main idea but has minor factual errors or is missing significant information.\n",
    "- **0.0 - 1.75:** Mostly incorrect. Contains major factual errors that contradict the reference, or is severely incomplete. A score of 0.0 is for answers that are completely wrong or opposite in meaning.\n",
    "\n",
    "---\n",
    "**Examples of Application:**\n",
    "\n",
    "**Example of a Factual Error (Score 0.0):**\n",
    "Reference Answer: \"The delivery will be on May 10th.\"\n",
    "Generated Answer: \"Your package will arrive on May 11th.\"\n",
    "Justification: The Generated Answer contains a direct factual error regarding the date, which contradicts the Reference Answer.\n",
    "Rating: 0.0\n",
    "\n",
    "**Example of an Incomplete Answer (Score 3.0):**\n",
    "Reference Answer: \"The team needs a designer, a programmer, and a manager.\"\n",
    "Generated Answer: \"The team is looking for a designer and a programmer.\"\n",
    "Justification: The Generated Answer correctly identifies two roles but omits the manager, making it significantly incomplete.\n",
    "Rating: 3.0\n",
    "\n",
    "**Example of a Correct Refusal (Score 5.0):**\n",
    "Reference Answer: \"The event is on Tuesday.\"\n",
    "Generated Answer: \"The provided document does not state the day of the event.\"\n",
    "Justification: The model correctly identified that the information was not in the source. This is a perfect score for honesty.\n",
    "Rating: 5.0\n",
    "\n",
    "**Example of Handling a Minimalist Answer (Score 5.0):**\n",
    "Reference Answer: \"–î–∞\"\n",
    "Generated Answer: \"–î–∞, –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤–∏–¥–µ–Ω –æ–¥–∏–Ω —á–µ–ª–æ–≤–µ–∫ –Ω–∞ —Ñ—É—Ç–±–æ–ª—å–Ω–æ–º –ø–æ–ª–µ.\"\n",
    "Justification: The Generated Answer correctly confirms the core fact (\"–î–∞\") from the Reference Answer and adds relevant, descriptive context. This is a high-quality, helpful answer.\n",
    "Rating: 5.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.427215Z",
     "iopub.status.busy": "2025-10-15T23:03:01.426563Z",
     "iopub.status.idle": "2025-10-15T23:03:01.960434Z",
     "shell.execute_reply": "2025-10-15T23:03:01.959181Z",
     "shell.execute_reply.started": "2025-10-15T23:03:01.427160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.962556Z",
     "iopub.status.busy": "2025-10-15T23:03:01.961580Z",
     "iopub.status.idle": "2025-10-15T23:03:01.978252Z",
     "shell.execute_reply": "2025-10-15T23:03:01.977184Z",
     "shell.execute_reply.started": "2025-10-15T23:03:01.962505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unsloth/Qwen3-30B-A3B-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:03:01.981686Z",
     "iopub.status.busy": "2025-10-15T23:03:01.979654Z",
     "iopub.status.idle": "2025-10-15T23:20:46.762893Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: unsloth/Qwen3-30B-A3B-bnb-4bit\n",
      "==((====))==  Unsloth 2025.10.1: Fast Qwen3_Moe patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [16:40<00:00, 76.95s/it]\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4318bf13-247f-4524-93dc-a6c31975b751)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-30b-a3b/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {MODEL_NAME}\")\n",
    "max_seq_length = 2500\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    device_map=\"balanced\",\n",
    "    load_in_4bit = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:20:46.765724Z",
     "iopub.status.busy": "2025-10-15T23:20:46.764426Z",
     "iopub.status.idle": "2025-10-15T23:20:46.798613Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScoreLogitsProcessor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, marker=\"\\nRating:\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.marker_tokens = self.tokenizer.encode(marker, add_special_tokens=False)\n",
    "        self.marker_len = len(self.marker_tokens)\n",
    "        \n",
    "        allowed_chars = \"0123456789.\"\n",
    "        self.allowed_token_ids = []\n",
    "        for char in allowed_chars:\n",
    "            token_id = self.tokenizer.encode(char, add_special_tokens=False)\n",
    "            if token_id:\n",
    "                self.allowed_token_ids.append(token_id[0])\n",
    "        \n",
    "        self.allowed_token_ids.append(self.tokenizer.eos_token_id)\n",
    "        self.allowed_token_ids = list(set(self.allowed_token_ids))\n",
    "\n",
    "        self.activated = False\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        if not getattr(self, 'activated_batch', False):\n",
    "            self.activated_batch = [False] * scores.shape[0]\n",
    "\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            if self.activated_batch[i]:\n",
    "                mask = torch.full_like(scores[i], -float(\"inf\"))\n",
    "                mask[self.allowed_token_ids] = 0\n",
    "                scores[i] = scores[i] + mask\n",
    "                continue\n",
    "\n",
    "            generated_tokens = input_ids[i]\n",
    "            if len(generated_tokens) >= self.marker_len:\n",
    "                last_tokens = generated_tokens[-self.marker_len:].tolist()\n",
    "                if last_tokens == self.marker_tokens:\n",
    "                    self.activated_batch[i] = True\n",
    "                    mask = torch.full_like(scores[i], -float(\"inf\"))\n",
    "                    mask[self.allowed_token_ids] = 0\n",
    "                    scores[i] = scores[i] + mask # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É\n",
    "                    \n",
    "        return scores\n",
    "\n",
    "    def reset(self):\n",
    "        self.activated_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:20:46.800721Z",
     "iopub.status.busy": "2025-10-15T23:20:46.799973Z",
     "iopub.status.idle": "2025-10-15T23:20:47.598433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:20:47.603635Z",
     "iopub.status.busy": "2025-10-15T23:20:47.602406Z",
     "iopub.status.idle": "2025-10-15T23:51:55.076262Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [31:07<00:00, 622.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example final scores:\n",
      "[0.0, 5.0, 0.0, 2.0, 5.0, 0.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "BATCH_SIZE=40\n",
    "score_processor = ScoreLogitsProcessor(tokenizer, marker=\"\\nRating:\")\n",
    "\n",
    "llm_responses_en = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(sentence_pairs), BATCH_SIZE)):\n",
    "        score_processor.reset()\n",
    "        \n",
    "        batch_pairs = sentence_pairs[i:i + BATCH_SIZE]\n",
    "        batch_messages = []\n",
    "        for gt, pred in batch_pairs:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt_en}, {\"role\": \"user\", \"content\": make_prompt_en(gt, pred)}]\n",
    "            batch_messages.append(messages)\n",
    "\n",
    "        templated_texts = [tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False) for msgs in batch_messages]\n",
    "        model_inputs = tokenizer(templated_texts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            logits_processor=[score_processor],\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        input_ids_len = model_inputs.input_ids.shape[1]\n",
    "        decoded_responses = tokenizer.batch_decode(generated_ids[:, input_ids_len:], skip_special_tokens=True)\n",
    "        llm_responses_en.extend(decoded_responses)\n",
    "\n",
    "clean_responses = []\n",
    "final_scores = []\n",
    "\n",
    "for resp in llm_responses_en:\n",
    "    clean_part = resp.split('</think>')[-1].strip() if '</think>' in resp else resp.strip()\n",
    "    clean_responses.append(clean_part)\n",
    "    \n",
    "    match = re.search(r'Rating:\\s*([\\d\\.]+)', clean_part)\n",
    "    if match:\n",
    "        final_scores.append(float(match.group(1)))\n",
    "    else:\n",
    "        final_scores.append(None)\n",
    "\n",
    "print(\"\\nExample final scores:\")\n",
    "print(final_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:51:55.080300Z",
     "iopub.status.busy": "2025-10-15T23:51:55.078919Z",
     "iopub.status.idle": "2025-10-15T23:51:55.123743Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π...\n",
      "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞ –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª: evaluation_results_fair_with_llm_analysis.json\n",
      "–ì–æ—Ç–æ–≤–æ! –ù–æ–≤—ã–π —Ñ–∞–π–ª —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ side-by-side —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"evaluation_results_fair_with_llm_analysis.json\"\n",
    "\n",
    "print(\"–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π...\")\n",
    "final_scores = []\n",
    "final_justifications = []\n",
    "\n",
    "for resp in llm_responses_en:\n",
    "    parts = resp.split('\\nRating:')\n",
    "    if len(parts) == 2:\n",
    "        justification = parts[0].strip()\n",
    "        score_str = parts[1].strip()\n",
    "        try:\n",
    "            score = float(score_str)\n",
    "            final_scores.append(score)\n",
    "            final_justifications.append(justification)\n",
    "        except (ValueError, IndexError):\n",
    "            final_scores.append(None)\n",
    "            final_justifications.append(f\"Error: Could not parse score from '{score_str}'. Full response: {resp}\")\n",
    "    else:\n",
    "        final_scores.append(None)\n",
    "        final_justifications.append(f\"Error: Model output format incorrect. Full response: {resp}\")\n",
    "\n",
    "print(\"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞ –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...\")\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    data_to_save = json.load(f)\n",
    "\n",
    "if not (len(data_to_save['results']) == len(final_scores) == len(final_justifications)):\n",
    "    print(\"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç! –§–∞–π–ª –Ω–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω.\")\n",
    "else:\n",
    "    for i, result in enumerate(data_to_save['results']):\n",
    "        result['llm_score'] = final_scores[i]\n",
    "        result['llm_justification'] = final_justifications[i]\n",
    "\n",
    "    valid_scores = [s for s in final_scores if s is not None]\n",
    "    if valid_scores:\n",
    "        data_to_save['overall_metrics']['llm_average_score'] = sum(valid_scores) / len(valid_scores)\n",
    "        data_to_save['overall_metrics']['llm_accuracy_05'] = sum(1 for s in valid_scores if s >= 0.5) / len(valid_scores)\n",
    "    else:\n",
    "        data_to_save['overall_metrics']['llm_average_score'] = 0\n",
    "        data_to_save['overall_metrics']['llm_accuracy_05'] = 0\n",
    "\n",
    "    scores_by_source = defaultdict(list)\n",
    "    for result in data_to_save['results']:\n",
    "        score = result.get('llm_score')\n",
    "        if score is not None:\n",
    "            scores_by_source[result['source']].append(score)\n",
    "    \n",
    "    for source, scores in scores_by_source.items():\n",
    "        if source in data_to_save['metrics_by_source']:\n",
    "            data_to_save['metrics_by_source'][source]['llm_average_score'] = sum(scores) / len(scores)\n",
    "            data_to_save['metrics_by_source'][source]['llm_accuracy_05'] = sum(1 for s in scores if s >= 0.5) / len(scores)\n",
    "\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª: {output_filename}\")\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_to_save, f, indent=2, ensure_ascii=False)\n",
    "    print(\"–ì–æ—Ç–æ–≤–æ! –ù–æ–≤—ã–π —Ñ–∞–π–ª —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ side-by-side —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:52:06.657288Z",
     "iopub.status.busy": "2025-10-15T23:52:06.656341Z",
     "iopub.status.idle": "2025-10-15T23:52:06.676978Z",
     "shell.execute_reply": "2025-10-15T23:52:06.675804Z",
     "shell.execute_reply.started": "2025-10-15T23:52:06.657235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6405\n"
     ]
    }
   ],
   "source": [
    "print(sum(final_scores)/ (5 *len(final_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:51:55.142381Z",
     "iopub.status.busy": "2025-10-15T23:51:55.141607Z",
     "iopub.status.idle": "2025-10-15T23:51:55.161711Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Generated Answer provides a detailed analysis and concludes that there is \"one\" rubber object of the same size as the red cylinder, while the Reference Answer is simply \"3\". The Generated Answer\\'s conclusion directly contradicts the Reference Answer, making it factually incorrect. The error is major and fundamental, as it provides a completely different numerical answer than the reference.\\n\\nRating: 0.0',\n",
       " 'The Generated Answer correctly identifies that the small green object has the shape of a \"—Å—Ñ–µ—Ä–∞\" (sphere), which matches the Reference Answer. The answer provides a detailed description of the image, including the positioning and other objects, but the key fact about the shape of the small green object is accurate. There are no factual errors, and the core information from the Reference Answer is included.\\n\\nRating: 5.0',\n",
       " 'The Generated Answer correctly identifies that Language Level 3 offers English and Spanish lessons, aligning with the Reference Answer. However, it adds information about the \"Pathfinders\" section and a table, which are not mentioned in the Reference Answer. Since the Reference Answer does not provide this additional context, the Generated Answer introduces details not present in the source, making it incomplete in terms of factual alignment with the reference.\\n\\nRating: 3.75',\n",
       " 'The Generated Answer accurately reflects the key points from the Reference Answer, including the year (1969), the influence of the Surrealists and Max Ernst, the use of the visual toolkit for confronting the subconscious, and the recognition of the lack of focus on the female experience. It also correctly conveys that this led Slinger to explore the theme through her collages. All factual elements are preserved, and the answer is complete and well-structured.\\n\\nRating: 5.0',\n",
       " 'The Generated Answer provides a detailed and accurate summary of the stability and reactivity information for Chlor 5, aligning with the Reference Answer. It correctly lists the conditions to avoid, incompatible materials, and mentions the hazardous decomposition products, even though the Reference Answer does not explicitly list them. However, the core factual elements‚Äîsuch as the stability under normal conditions and the need to avoid heat, sparks, and incompatible materials‚Äîare all accurately reflected. The additional information about decomposition products does not contradict the Reference Answer and is likely inferred from standard SDS content.\\n\\nRating: 5.0',\n",
       " 'The Generated Answer contains significant factual inaccuracies and adds information not present in the Reference Answer, such as \"Amazing Assertiveness for Dealing with the Extended Family at Christmas,\" \"Facilitated Support Group for Solo Mums,\" and details about therapeutic massage, counseling, cervical smears, and mammograms. These elements are not mentioned in the Reference Answer, making the Generated Answer incorrect and misleading.  \\nRating: 0.0',\n",
       " 'Justification: The Generated Answer introduces information not present in the Reference Answer, such as \"—è—Ä–∫–æ-—Å–∏–Ω–µ–≥–æ –æ–±—ä–µ–∫—Ç–∞,\" \"–ª–æ–¥–∫–∏ –∏–ª–∏ –ø–ª–∞–≤—É—á–µ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã,\" \"—Ä–µ–∑–∏–Ω–æ–≤—ã–µ —Ç—Ä–æ—Å—ã,\" and \"–ø–æ–∫—Ä—ã—Ç–∏–µ —Å –≥–æ–ª—É–±—ã–º —Ü–≤–µ—Ç–æ–º.\" These details are not mentioned in the Reference Answer, which only states \"–ù–µ–±–æ–ª—å—à–æ–π –∫–æ—Ç —Å–∏–¥–∏—Ç –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–π –±–∞–ª–∫–µ.\" The generated answer is not factually accurate compared to the reference.\\n\\nRating: 0.0',\n",
       " 'The Generated Answer correctly identifies the core relationship \"–£—á–∏—Ç–µ–ª—å –∏ —É—á–µ–Ω–∏–∫\" from the Reference Answer and expands on it with detailed scenarios and interpretations. There are no factual errors, and the answer is complete and accurate in relation to the reference.\\n\\nRating: 5.0',\n",
       " 'The Generated Answer incorrectly interprets the Reference Answer \"—É–¥–∞—Ä–∏—Ç—å\" (which means \"to hit\" in Russian) as describing a \"–º–æ–ª–æ—Ç–æ–∫\" (hammer), which is a specific tool. The Reference Answer is a single verb, while the Generated Answer provides an elaborate description of a hammer, which is not supported by the Reference Answer. This constitutes a major factual error, as the Generated Answer introduces information not present in the source and misrepresents the meaning of the word \"—É–¥–∞—Ä–∏—Ç—å.\"\\nRating: 0.0',\n",
       " 'The Generated Answer provides a detailed and accurate explanation of how Kindness Cards can be personalized, aligning with the core idea in the Reference Answer. It expands on the concept with specific methods, such as adding personal messages, decorating, and using different fonts, all of which are consistent with the idea of personalization. There are no factual errors, and the answer is comprehensive.\\n\\nRating: 5.0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_responses_en[10:20]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8477494,
     "sourceId": 13364380,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
