{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ad9aa5-493b-4977-bf11-583316e0c172",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-14T21:15:36.438066Z",
     "iopub.status.busy": "2025-10-14T21:15:36.436898Z",
     "iopub.status.idle": "2025-10-14T21:16:35.256502Z",
     "shell.execute_reply": "2025-10-14T21:16:35.255446Z",
     "shell.execute_reply.started": "2025-10-14T21:15:36.438016Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/jupyter/.local/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: timm in /home/jupyter/.local/lib/python3.10/site-packages (1.0.20)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/jupyter/.local/lib/python3.10/site-packages (0.35.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: safetensors in /home/jupyter/.local/lib/python3.10/site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.8.5)\n",
      "Requirement already satisfied: anyio in /home/jupyter/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jupyter/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/jupyter/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->timm) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->timm) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Installing collected packages: pyarrow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-21.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -q transformers==4.53.3\n",
    "%pip install einops timm datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5814e384-ace4-4e81-ad32-b5de817f43d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T07:05:53.509930Z",
     "iopub.status.busy": "2025-10-16T07:05:53.508498Z",
     "iopub.status.idle": "2025-10-16T07:06:18.784948Z",
     "shell.execute_reply": "2025-10-16T07:06:18.783946Z",
     "shell.execute_reply.started": "2025-10-16T07:05:53.509877Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from pathlib import Path\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c86eda-126f-43d3-aaf7-d9208b06bb55",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-14T21:22:08.729090Z",
     "iopub.status.busy": "2025-10-14T21:22:08.727785Z",
     "iopub.status.idle": "2025-10-14T21:22:29.242177Z",
     "shell.execute_reply": "2025-10-14T21:22:29.241202Z",
     "shell.execute_reply.started": "2025-10-14T21:22:08.729043Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==16.1.0\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==16.1.0) (1.22.4)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.2.0 requires pyarrow>=21.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-16.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade pyarrow==16.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d4553-bca3-48a7-afdf-6291cd92c8fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:28:17.587183Z",
     "iopub.status.busy": "2025-10-14T21:28:17.586235Z",
     "iopub.status.idle": "2025-10-14T21:28:17.602148Z",
     "shell.execute_reply": "2025-10-14T21:28:17.601175Z",
     "shell.execute_reply.started": "2025-10-14T21:28:17.587142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "user_site = os.path.expanduser(\"~/.local/lib/python3.10/site-packages\")\n",
    "\n",
    "if user_site not in sys.path:\n",
    "    sys.path.insert(0, user_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff21f41-9a95-4661-bdec-292d555cf272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T21:28:48.408290Z",
     "iopub.status.busy": "2025-10-14T21:28:48.406511Z",
     "iopub.status.idle": "2025-10-14T21:28:54.347185Z",
     "shell.execute_reply": "2025-10-14T21:28:54.346122Z",
     "shell.execute_reply.started": "2025-10-14T21:28:48.408237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "детергент\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8\", split=\"train\", streaming=True)\n",
    "print(next(iter(ds))['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cce1eb4-0ce6-4936-b781-4e24ac45a389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T18:00:44.522811Z",
     "iopub.status.busy": "2025-10-14T18:00:44.521779Z",
     "iopub.status.idle": "2025-10-14T20:01:52.227108Z",
     "shell.execute_reply": "2025-10-14T20:01:52.226215Z",
     "shell.execute_reply.started": "2025-10-14T18:00:44.522755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/project/kaggle.json'\n",
      "Downloading easy-grayscale-captcha200x40p-240k-plain-various.zip to ./captcha200x40p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1.18G/1.19G [00:42<00:00, 33.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.19G/1.19G [00:42<00:00, 29.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset downloaded to: ./captcha200x40p\n",
      "Contents: ['easy_grayscale_no_effects_system_fonts']\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ljhwild/easy-grayscale-captcha200x40p-240k-plain-various\"\n",
    "output_dir = \"./captcha200x40p\"\n",
    "os.system(f\"kaggle datasets download -d {dataset_name} -p {output_dir}\")\n",
    "zip_path = os.path.join(output_dir, f\"{dataset_name.split('/')[-1]}.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_dir)\n",
    "os.remove(zip_path)\n",
    "\n",
    "print(f\"Dataset downloaded to: {output_dir}\")\n",
    "print(\"Contents:\", os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9595b130-9be2-4f23-aff5-0c00673b28bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T20:29:54.919757Z",
     "iopub.status.busy": "2025-10-14T20:29:54.918109Z",
     "iopub.status.idle": "2025-10-14T20:50:42.378657Z",
     "shell.execute_reply": "2025-10-14T20:50:42.377811Z",
     "shell.execute_reply.started": "2025-10-14T20:29:54.919706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folders for .png + .txt pairs...\n",
      "💾 Saved DataFrame to: captha_word_dataset.csv\n",
      "\n",
      "First 5 rows:\n",
      "                                               image  ...                 source\n",
      "0  /home/jupyter/project/captcha200x40p/easy_gray...  ...  10ch_numbers_specials\n",
      "1  /home/jupyter/project/captcha200x40p/easy_gray...  ...  10ch_numbers_specials\n",
      "2  /home/jupyter/project/captcha200x40p/easy_gray...  ...      5ch_special_chars\n",
      "3  /home/jupyter/project/captcha200x40p/easy_gray...  ...      5ch_special_chars\n",
      "4  /home/jupyter/project/captcha200x40p/easy_gray...  ...  10ch_numbers_specials\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_dir = \"/home/jupyter/project/captcha200x40p/easy_grayscale_no_effects_system_fonts/\"\n",
    "def extract_letters_from_txt(txt_path):\n",
    "    \"\"\"Read .txt file, find TEXT= line, return only letters (Latin + Cyrillic).\"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = f.readline()[5:].strip()\n",
    "        return first_line\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "print(\"Scanning folders for .png + .txt pairs...\")\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(root_dir)\n",
    "data = []\n",
    "\n",
    "for png_path in root.rglob(\"*.png\"):\n",
    "    txt_path = png_path.with_suffix(\".txt\")\n",
    "    if txt_path.exists():\n",
    "        text = extract_letters_from_txt(str(txt_path))\n",
    "        if text:\n",
    "            data.append({\n",
    "                \"image\": str(png_path),\n",
    "                \"text\": text,\n",
    "                \"source\": png_path.parent.name\n",
    "            })\n",
    "df_captcha = pd.DataFrame(data)\n",
    "df_captcha = df_captcha.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "output_csv = \"captha_word_dataset.csv\"\n",
    "df_captcha.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved DataFrame to: {output_csv}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_captcha.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8c3ee-0634-48f1-affb-3de7d5048d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:36:57.266916Z",
     "iopub.status.busy": "2025-10-16T08:36:57.265134Z",
     "iopub.status.idle": "2025-10-16T08:36:57.286778Z",
     "shell.execute_reply": "2025-10-16T08:36:57.285639Z",
     "shell.execute_reply.started": "2025-10-16T08:36:57.266856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " def load_image_from_path(img_path):\n",
    "        return Image.open(img_path).convert(\"RGB\")\n",
    "def create_document(output,df_word,df_captha,directory = \"./train/\",num_documents = 1,min_words = 500,max_words = 600,min_captha = 5,max_captcha = 10,canvas_w =800 ,canvas_h = 2500):\n",
    "    metadata =  []\n",
    "    for doc_id in range(1, num_documents + 1):\n",
    "        word_count = random.randint(min_words, max_words)\n",
    "        captha_count = random.randint(min_captha, max_captcha)\n",
    "\n",
    "        captha_sample = df_captha.sample(n=captha_count)\n",
    "        word_sample = df_word.sample(n=word_count)\n",
    "        df_combined = pd.concat([captha_sample, word_sample], ignore_index=True).dropna().sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        full_text = full_text = \" \".join(df_combined['text'].astype(str))\n",
    "\n",
    "        document = Image.new(\"RGB\", (canvas_w, canvas_h), (255, 255, 255))\n",
    "    \n",
    "    \n",
    "        x = x_margin\n",
    "        y = y_start\n",
    "        current_line_height = 0\n",
    "        max_line_width = canvas_w - 2 * x_margin\n",
    "    \n",
    "        images = df_combined['image']\n",
    "        \n",
    "        for i, word_img in enumerate(images):\n",
    "        \n",
    "            scale_factor = 0.3\n",
    "            new_w = int(word_img.width * scale_factor)\n",
    "            new_h = int(word_img.height * scale_factor)\n",
    "            word_img = word_img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "            w, h = word_img.size\n",
    "            if x + w > x_margin + max_line_width:\n",
    "                x = x_margin\n",
    "                y += current_line_height + line_spacing\n",
    "                current_line_height = 0\n",
    "                if i % random.randint(5, 10) == 0 and random.random() < 0.3:\n",
    "                if y + 100 > canvas_h:\n",
    "                    break\n",
    "            document.paste(word_img, (x, y))\n",
    "            current_line_height = max(current_line_height, h)\n",
    "        final_height = min(y + current_line_height + 30, canvas_h)\n",
    "        document = document.crop((0, 0, canvas_w, final_height))\n",
    "        filename = f\"document_{doc_id:05d}_train.jpg\"\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        document.save(image_path, optimize=True, quality=90)\n",
    "        metadata.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"text\": full_text,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.to_csv(output, index=False, encoding=\"utf-8\")\n",
    "    print(f\"CSV saved to: metadata.csv\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Example text:\\n{df['text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bc2ea-900e-49e7-b6b6-53c5be536b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T20:56:52.066731Z",
     "iopub.status.busy": "2025-10-14T20:56:52.065376Z",
     "iopub.status.idle": "2025-10-14T20:56:52.178665Z",
     "shell.execute_reply": "2025-10-14T20:56:52.177299Z",
     "shell.execute_reply.started": "2025-10-14T20:56:52.066678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10c6579e2e6418193c37978468d0743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12aabcd-702f-4dc1-a92d-2aa2c8051b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T07:09:20.441650Z",
     "iopub.status.busy": "2025-10-16T07:09:20.440335Z",
     "iopub.status.idle": "2025-10-16T07:09:23.817631Z",
     "shell.execute_reply": "2025-10-16T07:09:23.816583Z",
     "shell.execute_reply.started": "2025-10-16T07:09:20.441609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8\", split=\"train\", streaming=True)\n",
    "subset = ds.take(500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d483b2-a769-4e7e-8267-d6a28128d923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T07:09:25.586589Z",
     "iopub.status.busy": "2025-10-16T07:09:25.585245Z",
     "iopub.status.idle": "2025-10-16T07:09:26.316254Z",
     "shell.execute_reply": "2025-10-16T07:09:26.315172Z",
     "shell.execute_reply.started": "2025-10-16T07:09:25.586530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_captha = pd.read_csv(\"/home/jupyter/project/captha_word_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95922862-bad1-4326-be78-31d644a17054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T07:09:41.430882Z",
     "iopub.status.busy": "2025-10-16T07:09:41.429491Z",
     "iopub.status.idle": "2025-10-16T07:33:58.547854Z",
     "shell.execute_reply": "2025-10-16T07:33:58.546885Z",
     "shell.execute_reply.started": "2025-10-16T07:09:41.430830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_captha['image'] = df_captha['image'].apply(load_image_from_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325225c-837e-4f09-9a8e-ccd18dcc224a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-16T07:33:58.550965Z",
     "iopub.status.busy": "2025-10-16T07:33:58.549780Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a5aa7702-ab4d-488e-b711-5e12768de31b)')' thrown while requesting GET https://huggingface.co/datasets/deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8/resolve/15cb45de49339ffaed2158e832259da96da4cd18/data/train-00000-of-00003.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 745f4dfa-f6d0-4d3b-ac45-848978b5bb21)')' thrown while requesting GET https://huggingface.co/datasets/deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8/resolve/15cb45de49339ffaed2158e832259da96da4cd18/data/train-00001-of-00003.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f2054a7d-0a2c-49d8-8624-63ba23dad56f)')' thrown while requesting GET https://huggingface.co/datasets/deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8/resolve/15cb45de49339ffaed2158e832259da96da4cd18/data/train-00001-of-00003.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 81b38315-875a-425d-8279-62baada1f8b9)')' thrown while requesting GET https://huggingface.co/datasets/deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8/resolve/15cb45de49339ffaed2158e832259da96da4cd18/data/train-00001-of-00003.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 23878547-debd-405f-9e56-9166ebab6258)')' thrown while requesting GET https://huggingface.co/datasets/deepcopy/DonkeySmall-OCR-Cyrillic-Printed-8/resolve/15cb45de49339ffaed2158e832259da96da4cd18/data/train-00001-of-00003.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    }
   ],
   "source": [
    "df_word = pd.DataFrame({\n",
    "            \"image\": subset['image'],\n",
    "            \"text\": subset['text'],\n",
    "            \"source\": \"donkey\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb844593-ea43-439e-9527-3716052dd139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:37:41.345562Z",
     "iopub.status.busy": "2025-10-16T08:37:41.344264Z",
     "iopub.status.idle": "2025-10-16T08:52:47.094051Z",
     "shell.execute_reply": "2025-10-16T08:52:47.093005Z",
     "shell.execute_reply.started": "2025-10-16T08:37:41.345514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: metadata.csv\n",
      "📊 Dataset shape: (7000, 2)\n",
      "📝 Example text:\n",
      "ПУБЛИЦИСТАМИ близится ОТКАЗАМИ купят УКЛАДЧИКОВ слэшеры Массагетов сейшелах ЭЭРО Экономичны Прерываниях Усатый гидродинамической дельфах внушаемых вольск нагревающихся филолог йеллоустонским аксиомати...\n"
     ]
    }
   ],
   "source": [
    "create_document(\"metadata_big_train.csv\",df_word[['image',\"text\"]],df_captha[['image',\"text\"]],directory = \"./train_Big/\",num_documents = 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b7d4de8-8847-41a4-bb8d-f07e6a43699f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:52:47.097256Z",
     "iopub.status.busy": "2025-10-16T08:52:47.096059Z",
     "iopub.status.idle": "2025-10-16T08:57:08.702776Z",
     "shell.execute_reply": "2025-10-16T08:57:08.701758Z",
     "shell.execute_reply.started": "2025-10-16T08:52:47.097192Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: metadata.csv\n",
      "📊 Dataset shape: (2000, 2)\n",
      "📝 Example text:\n",
      "ворончихин ТОКСИНАМ Военком Псам весенний излишнему неньки камминс Чика седьмому замереть Тарантула Пеннер невзрачного Трехминутных Повысит дёрнул дублинский автоконцерна локальное болгарку ПЕРЕСТРАХО...\n"
     ]
    }
   ],
   "source": [
    "create_document(\"metadata_big_val.csv\",df_word[['image',\"text\"]],df_captha[['image',\"text\"]],directory = \"./val_Big/\",num_documents = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0fa42-f05b-4d1c-bcd4-2fc2a76d8c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
