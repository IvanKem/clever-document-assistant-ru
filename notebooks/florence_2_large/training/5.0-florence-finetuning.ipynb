{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28b6b6b-8126-4072-873c-364695736798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T10:59:07.522811Z",
     "iopub.status.busy": "2025-10-16T10:59:07.522483Z",
     "iopub.status.idle": "2025-10-16T11:00:37.967043Z",
     "shell.execute_reply": "2025-10-16T11:00:37.965664Z",
     "shell.execute_reply.started": "2025-10-16T10:59:07.522791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-10-16 11:00:22.711317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-16 11:00:26.838626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM,AutoProcessor,get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b142f0-c214-4ee7-bd07-e35bdcc2577d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T00:12:22.521646Z",
     "iopub.status.busy": "2025-10-15T00:12:22.520887Z",
     "iopub.status.idle": "2025-10-15T00:14:20.580020Z",
     "shell.execute_reply": "2025-10-15T00:14:20.578691Z",
     "shell.execute_reply.started": "2025-10-15T00:12:22.521608Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -q transformers==4.53.3\n",
    "%pip install einops timm datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f208cea-5936-4f82-aeb6-94ec2bda4d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:00:37.969871Z",
     "iopub.status.busy": "2025-10-16T11:00:37.968546Z",
     "iopub.status.idle": "2025-10-16T11:00:37.982791Z",
     "shell.execute_reply": "2025-10-16T11:00:37.981785Z",
     "shell.execute_reply.started": "2025-10-16T11:00:37.969832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextRecognitionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        question = '<OCR>'\n",
    "        image = Image.open(f\"{row['image_path']}\").convert(\"RGB\")\n",
    "        labels = str(row['text'])\n",
    "        return question,image,labels\n",
    "def collate_fn(batch):\n",
    "    questions,images, labels = zip(*batch)\n",
    "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d79dc0a-340b-48c6-a38f-9016b002254c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:01:38.441235Z",
     "iopub.status.busy": "2025-10-16T11:01:38.440046Z",
     "iopub.status.idle": "2025-10-16T11:01:38.477172Z",
     "shell.execute_reply": "2025-10-16T11:01:38.476483Z",
     "shell.execute_reply.started": "2025-10-16T11:01:38.441211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, test_loader, model, processor, epochs=10, lr=1e-5, log_every=200):\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    max_length = 1024\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n",
    "            \n",
    "            inputs, answers = batch\n",
    "        \n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device, dtype=model_dtype)\n",
    "            labels = processor.tokenizer(\n",
    "            answers,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=False).input_ids.to(device)\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if (batch_idx + 1) % log_every == 0:\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, answers = batch\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                pixel_values = inputs[\"pixel_values\"].to(device, dtype=model_dtype)\n",
    "              \n",
    "                labels = processor.tokenizer(\n",
    "                    answers,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_token_type_ids=False\n",
    "                ).input_ids.to(device)\n",
    "\n",
    "                labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1} | Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            output_dir = f\"./modelFNEW_DOC_checkpoints_epoch{epoch+1}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model.save_pretrained(output_dir)\n",
    "            processor.save_pretrained(output_dir)\n",
    "            print(\"Saved new best model!\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, answers = batch\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device, dtype=model_dtype)\n",
    "\n",
    "            labels = processor.tokenizer(\n",
    "                answers,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_token_type_ids=False\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            test_loss += outputs.loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Final Test Loss: {avg_test_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942dac86-f1b8-4e8e-bea9-ba89dd5b1fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:01:41.321149Z",
     "iopub.status.busy": "2025-10-16T11:01:41.320632Z",
     "iopub.status.idle": "2025-10-16T11:01:45.129533Z",
     "shell.execute_reply": "2025-10-16T11:01:45.128774Z",
     "shell.execute_reply.started": "2025-10-16T11:01:41.321123Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/jupyter/project/metadata_big_train.csv\")\n",
    "df_val = pd.read_csv(\"/home/jupyter/project/metadata_big_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abd3d67-66e9-4479-a60e-f85d859fc809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:00:08.092276Z",
     "iopub.status.busy": "2025-10-15T13:00:08.089769Z",
     "iopub.status.idle": "2025-10-15T13:00:08.122646Z",
     "shell.execute_reply": "2025-10-15T13:00:08.121358Z",
     "shell.execute_reply.started": "2025-10-15T13:00:08.092218Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f4bcc8e4790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02c9b17-485c-417c-8f79-2631432546c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:01:49.503246Z",
     "iopub.status.busy": "2025-10-16T11:01:49.502633Z",
     "iopub.status.idle": "2025-10-16T11:01:49.532843Z",
     "shell.execute_reply": "2025-10-16T11:01:49.531712Z",
     "shell.execute_reply.started": "2025-10-16T11:01:49.503218Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TextRecognitionDataset(df)\n",
    "val_dataset = TextRecognitionDataset(df_val.iloc[:len(df_val)// 2])\n",
    "test_dataset = TextRecognitionDataset(df_val.iloc[len(df_val)// 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0c6c87-6048-4954-85f9-d12acea4f8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:01:50.826185Z",
     "iopub.status.busy": "2025-10-16T11:01:50.825624Z",
     "iopub.status.idle": "2025-10-16T11:01:50.889077Z",
     "shell.execute_reply": "2025-10-16T11:01:50.888264Z",
     "shell.execute_reply.started": "2025-10-16T11:01:50.826160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 14\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers,persistent_workers=True, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=8,persistent_workers=True,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=8,persistent_workers=True,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d605bc-e96e-40dc-b6c7-b13dc6936ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T11:03:10.009393Z",
     "iopub.status.busy": "2025-10-16T11:03:10.008701Z",
     "iopub.status.idle": "2025-10-16T11:04:07.931991Z",
     "shell.execute_reply": "2025-10-16T11:04:07.931044Z",
     "shell.execute_reply.started": "2025-10-16T11:03:10.009363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/jupyter/project/modelFBIG_DOC_checkpoints_epoch1\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b76523c-0fd7-4bb4-8764-b3e9c2965330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:15.208928Z",
     "iopub.status.busy": "2025-10-15T12:56:15.207875Z",
     "iopub.status.idle": "2025-10-15T12:56:15.257685Z",
     "shell.execute_reply": "2025-10-15T12:56:15.256691Z",
     "shell.execute_reply.started": "2025-10-15T12:56:15.208881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dtype = next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0994996d-70e1-4c75-9045-1093c7374bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:33:29.480781Z",
     "iopub.status.busy": "2025-10-15T11:33:29.479716Z",
     "iopub.status.idle": "2025-10-15T11:33:29.578026Z",
     "shell.execute_reply": "2025-10-15T11:33:29.577154Z",
     "shell.execute_reply.started": "2025-10-15T11:33:29.480729Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 15 11:33:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.261.03             Driver Version: 535.261.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              25W /  70W |   3225MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       874      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      2751      C   python3                                    3218MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef7b56-6917-4b08-b0ff-77d0bf9910ea",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-16T11:04:07.933664Z",
     "iopub.status.busy": "2025-10-16T11:04:07.933304Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  23%|██▎       | 200/875 [11:44<39:09,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 200/875 | Loss: 1.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  46%|████▌     | 400/875 [23:19<27:32,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 400/875 | Loss: 1.0480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  69%|██████▊   | 600/875 [34:55<15:56,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 600/875 | Loss: 0.8910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  91%|█████████▏| 800/875 [46:31<04:21,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 800/875 | Loss: 0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7: 100%|██████████| 875/875 [50:52<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Train Loss: 1.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/125 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 125/125 [02:24<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Val Loss: 0.6812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  23%|██▎       | 200/875 [11:36<39:12,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 200/875 | Loss: 0.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  46%|████▌     | 400/875 [23:12<27:34,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 400/875 | Loss: 0.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  69%|██████▊   | 600/875 [34:48<15:56,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 600/875 | Loss: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  91%|█████████▏| 800/875 [46:24<04:21,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 800/875 | Loss: 0.6156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7: 100%|██████████| 875/875 [50:45<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Train Loss: 0.6935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 125/125 [02:24<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Val Loss: 0.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  23%|██▎       | 200/875 [11:36<39:12,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 200/875 | Loss: 0.6073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  46%|████▌     | 400/875 [23:12<27:32,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 400/875 | Loss: 0.5660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  69%|██████▊   | 600/875 [34:48<15:56,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 600/875 | Loss: 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  91%|█████████▏| 800/875 [46:24<04:21,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 800/875 | Loss: 0.5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7: 100%|██████████| 875/875 [50:45<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Train Loss: 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 125/125 [02:24<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Val Loss: 0.4588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  23%|██▎       | 200/875 [11:36<39:13,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 200/875 | Loss: 0.5115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  46%|████▌     | 400/875 [23:12<27:33,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 400/875 | Loss: 0.4929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  69%|██████▊   | 600/875 [34:48<15:56,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 600/875 | Loss: 0.4666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  91%|█████████▏| 800/875 [46:25<04:21,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 800/875 | Loss: 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7: 100%|██████████| 875/875 [50:46<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Train Loss: 0.4834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 125/125 [02:24<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Val Loss: 0.4142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:  10%|▉         | 86/875 [04:59<45:48,  3.48s/it]"
     ]
    }
   ],
   "source": [
    "train_model(train_loader,val_loader,test_loader,model,processor, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eee89b6-d4ab-4f17-a731-4f15db0a0477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T19:41:14.671498Z",
     "iopub.status.busy": "2025-10-15T19:41:14.670984Z",
     "iopub.status.idle": "2025-10-15T19:41:15.116272Z",
     "shell.execute_reply": "2025-10-15T19:41:15.115296Z",
     "shell.execute_reply.started": "2025-10-15T19:41:14.671451Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory should be freed (but restart kernel to be sure)\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "model = None\n",
    "processor = None\n",
    "result_model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory should be freed (but restart kernel to be sure)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
